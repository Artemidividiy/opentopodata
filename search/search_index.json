{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"Open Topo Data <p> Open Topo Data is an elevation API. Host your own or use the free public API. </p> <p>Open Topo Data is a REST API server for your elevation data.</p> <pre><code>curl https://api.opentopodata.org/v1/test-dataset?locations=56,123\n</code></pre> <pre><code>{\n\"results\": [{\n\"elevation\": 815.0,\n\"location\": {\n\"lat\": 56.0,\n\"lng\": 123.0\n},\n\"dataset\": \"test-dataset\"\n}],\n\"status\": \"OK\"\n}\n</code></pre> <p>You can self-host with your own dataset or use the free public API which is configured with a number of open elevation datasets. The API is largely compatible with the Google Maps Elevation API.</p>"},{"location":"#host-your-own","title":"Host your own","text":"<p>Install docker and git then run:</p> <pre><code>git clone https://github.com/ajnisbet/opentopodata.git\ncd opentopodata\nmake build\nmake run\n</code></pre> <p>This will start an Open Topo Data server on <code>http://localhost:5000/</code>.</p> <p>Open Topo Data supports a wide range of raster file formats and tiling schemes, including most of those used by popular open elevation datasets.</p> <p>See the server docs for more about configuration, adding datasets, and running on Windows and M1 Macs.</p>"},{"location":"#usage","title":"Usage","text":"<p>Open Topo Data has a single endpoint: a point query endpoint that returns the elevation at a single point or a series of points.</p> <pre><code>curl https://api.opentopodata.org/v1/test-dataset?locations=56.35,123.90\n</code></pre> <pre><code>{\n\"results\": [{\n\"elevation\": 815.0,\n\"location\": {\n\"lat\": 56.0,\n\"lng\": 123.0\n},\n\"dataset\": \"test-dataset\"\n}],\n\"status\": \"OK\"\n}\n</code></pre> <p>The interpolation algorithm used can be configured as a request parameter, and locations can also be provided in Google Polyline format.</p> <p>See the API docs for more about request and response formats.</p>"},{"location":"#public-api","title":"Public API","text":"<p>I'm hosting a free public API at api.opentopodata.org.</p> <p>To keep the public API sustainable some limitations are applied.</p> <ul> <li>Max 100 locations per request.</li> <li>Max 1 call per second.</li> <li>Max 1000 calls per day.</li> </ul> <p>The following datasets are available on the public API, with elevation shown for downtown Denver, Colorado (39.7471,\u00a0-104.9963).</p> Dataset name Resolution Extent Source API link (Denver, CO) nzdem8m 8\u00a0m New Zealand. LINZ Not in dataset bounds ned10m ~10\u00a0m Continental USA, Hawaii, parts of Alaska. USGS 1590\u00a0m eudem25m 25\u00a0m Europe. EEA Not in dataset bounds mapzen ~30\u00a0m Global, inluding bathymetry. Mapzen 1590\u00a0m aster30m ~30\u00a0m Global. NASA 1591\u00a0m srtm30m ~30\u00a0m Latitudes -60 to 60. USGS 1604\u00a0m srtm90m ~90\u00a0m Latitudes -60 to 60. USGS 1603\u00a0m bkg200m 200\u00a0m Germany. BKG Not in dataset bounds etopo1 ~1.8\u00a0km Global, including bathymetry and ice surface elevation near poles. NOAA 1596\u00a0m gebco2020 ~450m Global bathymetry and land elevation. GEBCO 1603\u00a0m emod2018 ~100m Bathymetry for ocean and sea in Europe. EMODnet Not in dataset bounds <p>See the API docs for more about request formats and parameters.</p>"},{"location":"#support","title":"Support","text":"<p>Need some help getting Open Topo Data running? Send me an email at andrew@opentopodata.org!</p>"},{"location":"#paid-hosting","title":"Paid hosting","text":"<p>If you need an elevation API service with high-quality 1m lidar data, check out my sister project GPXZ.</p> <p>The GPXZ Elevation API offers the following features:</p> <ul> <li>Managed hosting, load balanced for redundancy</li> <li>Seamless, global, hi-res elevation dataset</li> <li>Drop-in replacement endpoint for the Google Maps Elevation API</li> <li>Priority support</li> <li>No hard usage limits</li> <li>EU-only servers if needed</li> <li>CORS (so you can use the API in your frontend webapp)</li> </ul> <p>For more details, reach out to andrew@opentopodata.org.</p> <p>Paid hosting funds the development of Open Topo Data and keeps the public API free.</p>"},{"location":"api/","title":"API Documentation","text":"<p>A public API is available for testing at api.opentopodata.org.</p>"},{"location":"api/#get-v1dataset_name","title":"<code>GET /v1/&lt;dataset_name&gt;</code>","text":"<p>Reads the elevation from a given dataset.</p> <p>The dataset name must match one of the options in <code>config.yaml</code>. </p> <p>Multiple datasets can be provided separated by commas: in this case, for each point, each dataset is queried in order until a non-null elevation is found. For more information see Multi datasets.</p> <p>Latitudes and longitudes should be in <code>EPSG:4326</code> (also known as WGS-84 format), they will be converted internally to whatever the dataset uses.</p>"},{"location":"api/#query-args","title":"Query Args","text":"<ul> <li><code>locations</code>: Required. Either <ul> <li><code>latitutde,longitude</code> pairs, each separated by a pipe character <code>|</code>. Example: <code>locations=12.5,160.2|-10.6,130</code>.</li> <li>Google polyline format. Example: <code>locations=gfo}EtohhU</code>.</li> </ul> </li> <li><code>samples</code>: If provided, instead of using <code>locations</code> directly, query elevation for <code>sample</code> equally-spaced points along the path specified by <code>locations</code>. Example: <code>samples=5</code>.</li> <li><code>interpolation</code>: How to interpolate between the points in the dataset. Options: <code>nearest</code>, <code>bilinear</code>, <code>cubic</code>. Default: <code>bilinear</code>.</li> <li><code>nodata_value</code>: What elevation to return if the dataset has a NODATA value at the requested location. Options: <code>null</code>, <code>nan</code>, or an integer like <code>-9999</code>. Default: <code>null</code>.<ul> <li>The default option <code>null</code> makes NODATA indistinguishable from a location outside the dataset bounds. </li> <li><code>NaN</code> (not a number) values aren't valid in json and will break some clients. The <code>nan</code> option was default before version 1.4 and is provided only for backwards compatibility. </li> <li>When querying multiple datasets, this NODATA replacement only applies to the last dataset in the stack.</li> </ul> </li> </ul>"},{"location":"api/#response","title":"Response","text":"<p>A json object, compatible with the Google Maps Elevation API.</p> <ul> <li><code>status</code>: Will be <code>OK</code> for a successful request, <code>INVALID_REQUEST</code> for an input (4xx) error, and <code>SERVER_ERROR</code> for anything else (5xx). Required.</li> <li><code>error</code>: Description of what went wrong, when <code>status</code> isn't <code>OK</code>.</li> <li><code>results</code>: List of elevations for each location, in same order as input. Only provided for <code>OK</code> status.</li> <li><code>results[].elevation</code>: Elevation, using units and datum from the dataset. Will be <code>null</code> if the given location is outside the dataset bounds. May be <code>null</code> for NODATA values depending on the <code>nodata_value</code> query argument.</li> <li><code>results[].location.lat</code>: Latitude as parsed by Open Topo Data.</li> <li><code>results[].location.lng</code>: Longitude as parsed by Open Topo Data.</li> <li><code>results[].dataset</code>: The name of the dataset which the returned elevation is from.</li> </ul> <p>Some notes about the elevation value:</p> <ul> <li>If the raster has an integer data type, the interpolated elevation will be rounded to the nearest integer. This is a limitation of rasterio/gdal.</li> <li>If the request location isn't covered by any raster in the dataset, Open Topo Data will return <code>null</code>.</li> <li>Unless the <code>nodata_value</code> parameter is set, a <code>null</code> elevation could either mean the location is outside the dataset bounds, or a NODATA within the raster bounds. </li> </ul>"},{"location":"api/#example","title":"Example","text":"<p><code>GET</code> api.opentopodata.org/v1/srtm90m?locations=-43.5,172.5|27.6,1.98&amp;interpolation=cubic</p> <pre><code>{\n\"results\": [\n{\n\"dataset\": \"srtm90m\",\n\"elevation\": 45,\n\"location\": {\n\"lat\": -43.5,\n\"lng\": 172.5\n}\n},\n{\n\"dataset\": \"srtm90m\",\n\"elevation\": 402,\n\"location\": {\n\"lat\": 27.6,\n\"lng\": 1.98\n}\n}\n],\n\"status\": \"OK\"\n}\n</code></pre>"},{"location":"api/#post-v1dataset_name","title":"<code>POST /v1/&lt;dataset_name&gt;</code>","text":"<p>When querying many locations in a single request, you can run into issues fitting them all in one url. To avoid these issues, you can also send a POST request to <code>/v1/&lt;dataset_name&gt;</code>.</p> <p>The arguments are the same, but must be provided either as json-encoded data or form data instead of url query parameters.</p> <p>The response is the same.</p> <p>Other solutions for fitting many points in a URL are polyline encoding and rounding your coordinates.</p>"},{"location":"api/#example_1","title":"Example","text":"<p>With json:</p> <pre><code>import requests\nurl = \"https://api.opentopodata.org/v1/srtm90m\"\ndata = {\n\"locations\": \"-43.5,172.5|27.6,1.98\",\n\"interpolation\": \"cubic\",\n}\nresponse = requests.post(url json=data)\n</code></pre> <p>With form data:</p> <pre><code>import requests\nurl = \"https://api.opentopodata.org/v1/srtm90m\"\ndata = {\n\"locations\": \"-43.5,172.5|27.6,1.98\",\n\"interpolation\": \"cubic\",\n}\nresponse = requests.post(url data=data)\n</code></pre> <p>The response is the same as for GET requests:</p> <pre><code>{\n\"results\": [\n{\n\"dataset\": \"srtm90m\",\n\"elevation\": 45,\n\"location\": {\n\"lat\": -43.5,\n\"lng\": 172.5\n}\n},\n{\n\"dataset\": \"srtm90m\",\n\"elevation\": 402,\n\"location\": {\n\"lat\": 27.6,\n\"lng\": 1.98\n}\n}\n],\n\"status\": \"OK\"\n}\n</code></pre>"},{"location":"api/#get-health","title":"<code>GET /health</code>","text":"<p>Healthcheck endpoint, for use with load balancing or monitoring.</p>"},{"location":"api/#response_1","title":"Response","text":"<p>A json object.</p> <ul> <li><code>status</code>: Will be <code>OK</code> for a successful request.</li> </ul> <p>The status code is 200 if healthy, otherwise 500.</p>"},{"location":"api/#example_2","title":"Example","text":"<p><code>GET</code> api.opentopodata.org/health</p> <pre><code>{\n    \"status\": \"OK\"\n}\n</code></pre>"},{"location":"api/#get-datasets","title":"<code>GET /datasets</code>","text":"<p>Details of the datasets available on the server.</p>"},{"location":"api/#response_2","title":"Response","text":"<p>A json object.</p> <ul> <li><code>datasets</code>: List of datasets.</li> <li><code>datasets[].name</code>: Dataset name, used in the elevation query URL.</li> <li><code>datasets[].child_datasets</code>: If the dataset is a MultiDataset, names of the child datasets. Otherwise, an empty list <code>[]</code>.</li> <li><code>status</code>: Will be <code>OK</code> if the server is running and the config file can be loaded. Otherwise the value will be <code>SERVER_ERROR</code>.</li> </ul>"},{"location":"api/#example_3","title":"Example","text":"<p><code>GET</code> api.opentopodata.org/datasets</p> <pre><code>{\n    \"results\": [\n        {\n            \"child_datasets\": [],\n            \"name\": \"test-dataset\"\n        }\n    ]\n    \"status\": \"OK\"\n}\n</code></pre>"},{"location":"changelog/","title":"Release Notes","text":"<p>This is a list of changes to Open Topo Data between each release.</p>"},{"location":"changelog/#version-183-7-feb-2023","title":"Version 1.8.3 (7 Feb 2023)","text":"<ul> <li>Fix memory leak (#68)</li> <li>Fix invalid file error message (#70)</li> <li>Downgrade to python 3.9 and rasterio 1.2.10</li> </ul>"},{"location":"changelog/#version-182-7-nov-2022","title":"Version 1.8.2 (7 Nov 2022)","text":"<ul> <li>Fix broken Docker image (#66)</li> <li>Fix docs code highlighting</li> <li>Add Swisstopo documentation</li> <li>Get pylibmc from pypi instead of building from source (simplifying docker image)</li> <li>Upgrade uwsgi (to ease pathway to python 3.10)</li> <li>Siginficant dependency upgrades, including python to 3.10</li> </ul>"},{"location":"changelog/#version-181-26-jul-2022","title":"Version 1.8.1 (26 Jul 2022)","text":"<ul> <li>Better quoting in Makefile.</li> <li>Dependency updates including <code>rasterio</code> to 1.3.0, and patch upgrades to docker base images.</li> </ul>"},{"location":"changelog/#version-180-4-apr-2022","title":"Version 1.8.0 (4 Apr 2022)","text":"<ul> <li>Added datasets endpoint.</li> <li>Add <code>CONFIG_PATH</code> environment variable.</li> <li>Minor dependency updates.</li> </ul>"},{"location":"changelog/#version-172-16-feb-2022","title":"Version 1.7.2 (16 Feb 2022)","text":"<ul> <li>Instructions for running on kubernetes, thanks to @khintz.</li> <li>Updated main docker image from Python 3.7 to 3.9, and Debian from Buster to Bullseye.</li> <li>Minor dependency updates.</li> </ul>"},{"location":"changelog/#version-171-13-nov-2021","title":"Version 1.7.1 (13 Nov 2021)","text":"<ul> <li>Support Apple Silicon M1 macs (#55).</li> <li>Minor dependency updates.</li> </ul>"},{"location":"changelog/#version-170-7-oct-2021","title":"Version 1.7.0 (7 Oct 2021)","text":"<ul> <li>Support POST requests (#49).</li> <li>Dataset name can no longer be null for multi-dataset requests.</li> </ul>"},{"location":"changelog/#version-160-7-sep-2021","title":"Version 1.6.0 (7 Sep 2021)","text":"<ul> <li>Added support for samples along a path (#37).</li> <li>Added version response header (#47).</li> <li>Enabled cors in example config file.</li> </ul>"},{"location":"changelog/#version-152-17-aug-2021","title":"Version 1.5.2 (17 Aug 2021)","text":"<ul> <li>Updated dependencies.</li> </ul>"},{"location":"changelog/#version-151-28-apr-2021","title":"Version 1.5.1 (28 Apr 2021)","text":"<ul> <li>Updated dependencies, including to rasterio 1.2.3.</li> <li>Fix some typos in scripts in the documentation.</li> </ul>"},{"location":"changelog/#version-150-5-feb-2021","title":"Version 1.5.0 (5 Feb 2021)","text":"<ul> <li>Big performance improvements, thanks to caching expensive coordinate transforms, reducing the need to deserialise cached objects, and scaling processes to the machine being used.</li> <li>Add BKG data.</li> <li>Add Multiple Dataset feature.</li> <li>Updated rasterio and pyproj dependencies.</li> </ul>"},{"location":"changelog/#version-141-10-dec-2020","title":"Version 1.4.1 (10 Dec 2020)","text":"<ul> <li>Increased max URI length.</li> <li>Support datasets with <code>.prj</code> files.</li> <li>Docs fixes. </li> <li>Small dependency updates.</li> </ul>"},{"location":"changelog/#version-140-9-nov-2020","title":"Version 1.4.0 (9 Nov 2020)","text":"<ul> <li>Fixes bug #13 where responses could return invalid json. This changes the NODATA value from <code>NaN</code> to <code>null</code>. The old behaviour can be enabled by sending a <code>nodata_value=nan</code> query parameter.</li> <li>Small dependency updates.</li> </ul>"},{"location":"changelog/#version-131-23-oct-2020","title":"Version 1.3.1 (23 Oct 2020)","text":"<ul> <li>Improved code style and test coverage.</li> <li>Updated dependencies.</li> <li>Documentation for running on Windows.</li> </ul>"},{"location":"changelog/#version-130-4-sep-2020","title":"Version 1.3.0 (4 Sep 2020)","text":"<ul> <li>Added <code>/health</code> endpoint.</li> </ul>"},{"location":"changelog/#version-124-8-aug-2020","title":"Version 1.2.4 (8 Aug 2020)","text":"<ul> <li>Support for more raster filename formats, as raised in issue #8.</li> </ul>"},{"location":"changelog/#version-123-31-july-2020","title":"Version 1.2.3 (31 July 2020)","text":"<ul> <li>Minor documentation fixes and dependency updates.</li> </ul>"},{"location":"changelog/#version-122-2-july-2020","title":"Version 1.2.2 (2 July 2020)","text":"<ul> <li>Documentation fixes.</li> <li>I'm now using pip-tools to manage python dependencies, and I'm really liking it. Exact dependency versions are now pinned, but it's easy to update them to the latest version.</li> <li>Updated dependencies. </li> </ul>"},{"location":"changelog/#version-121-22-may-2020","title":"Version 1.2.1 (22 May 2020)","text":"<p>Improved documentation, plus some bug fixes:</p> <ul> <li>Fix floating-point precision issue that was breaking requests on dataset boundaries.</li> <li>Ignore common non-raster files.</li> </ul>"},{"location":"changelog/#version-120-10-may-2020","title":"Version 1.2.0 (10 May 2020)","text":"<p>Added a bunch of new datasets to the public API:</p> <ul> <li>GEBCO bathymetry.</li> <li>EMOD bathymetry.</li> <li>NZ 8m DEM.</li> <li>Mapzen 30m DEM.</li> </ul>"},{"location":"changelog/#version-110-25-april-2020","title":"Version 1.1.0 (25 April 2020)","text":"<ul> <li>Added this changelog! Pushing to master now means a release with a changelog entry.</li> <li>Added VERSION.txt, docker images get tagged with version when built.</li> <li>Documented install instructions and a brief overview for the datasets used in the public API.</li> <li>Makefile improvements (suggested by a user, thank you!).</li> <li>Increased the public API daily request limit to 1000.</li> <li>Updated NED on the public API.</li> <li>Added CORS header support.</li> </ul>"},{"location":"server/","title":"Open Topo Data Server Documentation","text":""},{"location":"server/#getting-started","title":"Getting started","text":"<p>The easiest way to run Open Topo Data is with Docker. Install docker then run the following commands:</p> <pre><code>git clone https://github.com/ajnisbet/opentopodata.git\ncd opentopodata\nmake build\nmake run\n</code></pre> <p>This will start a server on <code>localhost:5000</code> with a small demo dataset called <code>test-dataset</code>. Check out the API docs for info about the format of requests and responses.</p>"},{"location":"server/#getting-started-on-m1-apple-silicon-macs","title":"Getting started on M1 / Apple Silicon Macs","text":"<p>On M1 Macs you'll probably need to use the alternate <code>apple-silicon.Dockerfile</code> docker image, which includes build dependencies for libraries that don't have binaries for M1.</p> <pre><code>git clone https://github.com/ajnisbet/opentopodata.git\ncd opentopodata\nmake build-m1\nmake run\n</code></pre> <p>This should work without rosetta. See issue #55 for more info.</p>"},{"location":"server/#getting-started-on-windows","title":"Getting started on Windows","text":"<p>On Windows you'll probably need to run the build and run commands without make:</p> <pre><code>git clone https://github.com/ajnisbet/opentopodata.git\ncd opentopodata\ndocker build --tag opentopodata --file docker/Dockerfile . docker run --rm -it --volume C:/path/to/opentopodata/data:/app/data:ro -p 5000:5000 opentopodata sh -c \"/usr/bin/supervisord -c /app/docker/supervisord.conf\"\n</code></pre> <p>For more details see this note on windows support.</p>"},{"location":"server/#dataset-support","title":"Dataset support","text":"<p>Open Topo Data supports all georeferenced raster formats supported by GDAL (e.g, <code>.tiff</code>, <code>.hgt</code>, <code>.jp2</code>).</p> <p>Datasets can take one of two formats:</p> <ul> <li>A single raster file.</li> <li>A collection of square raster tiles which follow the SRTM naming convention: the file is named for the lower left corner. So a file named <code>N30W120.tiff</code> would span from 30 to 31 degrees latitude, and -120 to -119 degrees longitude. By default tiles are 1\u00b0 by 1\u00b0 and the coordinates are in WGS84, but this can be configured.</li> </ul> <p>If your dataset consists of multiple files that aren't on a nice grid, you can create a <code>.vrt</code> file pointing to the files that Open Topo Data will treat as a single-file dataset. For an example of this process, see the documentation for configuring EMODnet.</p>"},{"location":"server/#configuration","title":"Configuration","text":"<p>Open Topo Data is configured by a <code>config.yaml</code> file. If that file is missing it will look for <code>example-config.yaml</code> instead. You can set the <code>CONFIG_PATH</code> environment variable to specify a different path.</p> <p>A config might look like:</p> <pre><code>max_locations_per_request: 100 access_control_allow_origin: '*'\ndatasets:\n- name: etopo1\npath: data/etopo1/\n- name: srtm90m\npath: data/srtm-90m-v3/\nfilename_epsg: 4326\nfilename_tile_size: 1\n</code></pre> <p>corresponding to a directory structure:</p> <pre><code>opentopodata\n|\n\u2514\u2500\u2500\u2500data\n    |\n    \u251c\u2500\u2500\u2500etopo1\n    |   |\n    |   \u2514\u2500\u2500\u2500etopo1-dem.geotiff\n    |\n    \u2514\u2500\u2500\u2500srtm-90m-v3\n        |\n        \u251c\u2500\u2500\u2500N00E000.hgt \n        \u251c\u2500\u2500\u2500N00E001.hgt \n        \u251c\u2500\u2500\u2500N00E002.hgt \n        \u251c\u2500\u2500\u2500etc...\n</code></pre> <p>which would expose <code>localhost:5000/v1/etopo1</code> and <code>localhost:5000/v1/srtm90m</code>.</p>"},{"location":"server/#config-spec","title":"Config spec","text":"<ul> <li><code>max_locations_per_request</code>: Requests with more than this many locations will return a 400 error. Default: <code>100</code>.</li> <li><code>access_control_allow_origin</code>: Value for the <code>Access-Control-Allow-Origin</code> CORS header. Set to <code>*</code> or a domain to allow in-browser requests from a different origin. Set to <code>null</code> to send no <code>Access-Control-Allow-Origin</code> header. Default: <code>null</code>.</li> <li><code>datasets[].name</code>: Dataset name, used in url. Required.</li> <li><code>datasets[].path</code>: Path to folder containing the dataset. If the dataset is a single file it must be placed inside a folder. This path is relative to the repository directory inside docker. I suggest placing datasets inside the provided <code>data</code> folder, which is mounted in docker by <code>make run</code>. Files can be nested arbitrarily inside the dataset path. Required.</li> <li><code>datasets[].filename_epsg</code>: For tiled datasets, the projection of the filename coordinates. The default value is <code>4326</code>, which is latitude/longitude with the WGS84 datum.</li> <li><code>datasets[].filename_tile_size</code>: For tiled datasets, how large each square tile is in the units of <code>filename_epsg</code>. For example, a lat,lon location of <code>38.2,121.2</code> would lie in the tile <code>N38W121</code> for a tile size of 1, but lie in <code>N35W120</code> for a tile size of 5. For non-integer tile sizes like <code>2.5</code>, specify them as a string to avoid floating point parsing issues: <code>\"2.5\"</code>. Default: <code>1</code>.</li> <li><code>datasets[].wgs84_bounds.left</code>: Leftmost (westmost) longitude of the dataset, in WGS84. Used as a performance optimisation for Multi datasets. Default: <code>-180</code>.</li> <li><code>datasets[].wgs84_bounds.right</code>: Rightmost (eastmost) longitude of the dataset. Default: <code>180</code>.</li> <li><code>datasets[].wgs84_bounds.bottom</code>: Bottommost (southmost) latitude of the dataset. Default: <code>-90</code>.</li> <li><code>datasets[].wgs84_bounds.top</code>: Topmost (northmost) latitude of the dataset. Default: <code>90</code>.</li> <li><code>datasets[].child_datasets[]</code>: A list of names of other datasets. Querying this MultiDataset will check each dataset in <code>child_datasets</code> in order until a non-null elevation is found. For more information see Multi datasets. </li> </ul>"},{"location":"server/#adding-datasets","title":"Adding datasets","text":"<p>An important goal of Open Topo Data is make is easy to add new datasets. The included dataset is very low resolution (etopo1 downsampled to about 100km) and is intended only for testing.</p> <p>Adding a new dataset takes two steps:</p> <ol> <li>placing the dataset in the <code>data</code> directory</li> <li>adding the path to the dataset in <code>config.yaml</code>.</li> </ol> <p>Instructions are provided for adding the various datasets used in the public API:</p> <ul> <li>ASTER</li> <li>ETOPO1</li> <li>EU-DEM</li> <li>Mapzen</li> <li>NED 10m</li> <li>NZ DEM</li> <li>SRTM (30m or 90m)</li> <li>EMOD Bathymetry</li> <li>GEBCO Bathymetry</li> <li>BKG Germany (200m)</li> </ul>"},{"location":"server/#kubernetes","title":"Kubernetes","text":"<p>See How to deploy on Kubernetes for details and config files for running on kubernetes.</p>"},{"location":"datasets/aster/","title":"ASTER","text":""},{"location":"datasets/aster/#overview","title":"Overview","text":"<p>The Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) global DEM dataset is a joint effort between the Ministry of Economy, Trade, and Industry (METI) of Japan and the National Aeronautics and Space Administration (NASA) of the US.</p> <p>ASTER GDEM is a 1 arc-second resolution, corresponding to a resolution of about 30m at the equator. Coverage is provided from from -83 to 83 degrees latitude.</p> <p> Render of ASTER elevation. Source. </p>"},{"location":"datasets/aster/#adding-30m-aster-to-open-topo-data","title":"Adding 30m ASTER to Open Topo Data","text":"<p>Make a new folder for the dataset:</p> <pre><code>mkdir ./data/aster30m\n</code></pre> <p>Download the files from USGS into <code>./data/aster30m</code> . Extract the zip archives keeping the <code>_dem.tif</code> files and removing the <code>_num.tif</code> files.</p> <p>To make downloading a bit easier, here's a list of the 22,912 URLs: aster30m_urls.txt.</p> <p>Create a <code>config.yaml</code> file:</p> <pre><code>datasets:\n- name: aster30m\npath: data/aster30m/\n</code></pre> <p>Rebuild to enable the new dataset at localhost:5000/v1/aster30m.</p> <pre><code>make build &amp;&amp; make run\n</code></pre>"},{"location":"datasets/aster/#public-api","title":"Public API","text":"<p>The Open Topo Data public API lets you query ASTER GDEM 30m for free:</p> <pre><code>curl https://api.opentopodata.org/v1/aster30m?locations=57.688709,11.976404\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"elevation\": 45.0, \"location\": {\n\"lat\": 57.688709, \"lng\": 11.976404\n},\n\"dataset\": \"aster30m\"\n}\n], \"status\": \"OK\"\n}\n</code></pre> <p>The Public API uses version 3 of the DEM (GDEM 003).</p>"},{"location":"datasets/bkg/","title":"BKG","text":"<p>BKG (Bundesamt f\u00fcr Kartographie und Geod\u00e4sie) has published a number of elevation datasets for Germany. The 200m and 1000m resolutions are freely available, while resolutions up to 5m can be purchased for a fee.</p> <p> Render of BKG 200m DTM elevation. </p>"},{"location":"datasets/bkg/#adding-200m-bkg-digital-terrain-model-to-open-topo-data","title":"Adding 200m BKG Digital Terrain Model to Open Topo Data","text":"<p>Make a new folder for the dataset:</p> <pre><code>mkdir ./data/bkg200m\n</code></pre> <p>Download the dataset, the link here is for the UTM referenced dataset in <code>.asc</code> format. Extract the zip and copy only <code>dgm200_utm32s.asc</code> and <code>dgm200_utm32s.prj</code> into <code>./data/bkg200m</code>. </p> <p>Add the dataset to <code>config.yaml</code>:</p> <pre><code>- name: bkg200m\npath: data/bkg200m\n</code></pre> <p>Finally, rebuild to enable the new dataset at localhost:5000/v1/bkg200m?locations=49.427,7.753.</p> <pre><code>make build &amp;&amp; make run\n</code></pre>"},{"location":"datasets/bkg/#adding-5m-germany-dem-to-open-topo-data","title":"Adding 5m Germany DEM to Open Topo Data","text":"<p>I don't have access to the higher-resolution datasets, but one user managed to get them working with Open Topo Data in issues #22 and #24.</p> <p>The files come with a projection format that isn't supported by GDAL and with non-overlapping tiles so queries near the edges will return <code>null</code> data. Probably the easiest way to handle these issues is to merge all the files into one big geotiff:</p> <pre><code>gdalbuildvrt -tap -a_srs epsg:3044 -o bkg-dgm5.vrt dgm5/*.asc\ngdaltranslate -co COMPRESS=DEFLATE -co BIGTIFF=YES -co NUM_THREADS=ALL_CPUS bkg-dgm5.vrt bkg-dgm5.tif\n</code></pre> <p>You could also add a buffer to each tile, fixing the projection with <code>-a_srs epsg:3044</code>.</p> <p>If anyone has got this working and would like to share their steps, please open an issue or pull request!</p>"},{"location":"datasets/bkg/#public-api","title":"Public API","text":"<p>The Open Topo Data public API lets you query 200m BKG DEM over Germany for free:</p> <pre><code>curl https://api.opentopodata.org/v1/bkg200m?locations=49.427,7.753\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"elevation\": 269.9404602050781, \"location\": {\n\"lat\": 49.427, \"lng\": 7.753\n},\n\"dataset\": \"bkg200m\"\n}\n], \"status\": \"OK\"\n}\n</code></pre>"},{"location":"datasets/bkg/#attribution","title":"Attribution","text":"<p>\u00a9 GeoBasis-DE / BKG (Jahr des Datenbezugs)</p> <p>Datenlizenz Deutschland \u2013 Namensnennung \u2013 Version 2.0</p>"},{"location":"datasets/emod2018/","title":"EMODnet 2018 Bathymetry","text":"<p>EMODnet maintains a number of bathymetry (sea floor depth) datasets.</p> <p>There are currently two datasets with full coverage of Europe: a 1/8 arc minute (~200m) version released in 2016, and a 1/16 arc minute (~100m) version released in 2018. The datasets are a composite of bathymetric surveys, Landsat 8 imagery, and GEBCO data.</p> <p>The vertical datum used is Lowest Astronomical Tide.</p>"},{"location":"datasets/emod2018/#coverage","title":"Coverage","text":"<p>The 2018 dataset covers a large area around Europe: 15\u00b0 to 90\u00b0 latitude, and -36\u00b0 to 43\u00b0 longitude. The elevation over land areas is given as <code>NODATA</code> values.</p> <p> Render of EMODnet 2018 sea floor depth. </p>"},{"location":"datasets/emod2018/#adding-emodnet-2018-bathymetry-to-open-topo-data","title":"Adding EMODnet 2018 Bathymetry to Open Topo Data","text":"<p>Make a new folder for the dataset:</p> <pre><code>mkdir ./data/emod2018\n</code></pre> <p>Download the files from EMODnet into <code>./data/emod2018</code>. You want the 2018 dataset, in <code>ESRI ASCII</code> format. Extract the zip folders, you should have 59 <code>.asc</code> named like <code>A1_2018.asc</code>.</p> <p>Unlike other datasets, the EMOD tiles aren't aligned on a nice whole-number grid, so Open Topo Data can't tell from the filenames which tile covers which area. To handle this we can build a https://gdal.org/drivers/raster/vrt.html - a single raster file that links to the 59 tiles and which Open Topo Data can treat as a single-file dataset. Unfortunately you'll need to install GDAL for this.</p> <p>Create a new folder for the VRT: <pre><code>mkdir ./data/emod2018-vrt\n</code></pre></p> <p>Build the vrt using relative paths so the file will work inside the docker image: <pre><code>cd ./data/emod2018-vrt\ngdalbuildvrt -a_srs epsg:4326 emod2018.vrt ../emod2018/*.asc\ncd ../\n</code></pre></p> <p>Create a <code>config.yaml</code> file, pointing to the VRT folder:</p> <pre><code>datasets:\n- name: emod2018\npath: data/emod2018-vrt/\n</code></pre> <p>Rebuild to enable the new dataset at localhost:5000/v1/emod2018.</p> <pre><code>make build &amp;&amp; make run\n</code></pre> <p>Extra performance</p> <p><code>.asc</code> files take up a lot of disk space and are slow for random reads. Consider converting to a compressed geotiff:</p> <pre><code>gdal_translate -co COMPRESS=DEFLATE -a_srs epsg:4326 {asc_filename} {tif_filename}\n</code></pre>"},{"location":"datasets/emod2018/#public-api","title":"Public API","text":"<p>The Open Topo Data public API lets you query NED 10m for free:</p> <pre><code>curl https://api.opentopodata.org/v1/emod2018?locations=55.884323,2.528276\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"elevation\": -81.94999694824219, \"location\": {\n\"lat\": 55.884323, \"lng\": 2.528276\n},\n\"dataset\": \"emod2018\"\n}\n], \"status\": \"OK\"\n}\n</code></pre> <p>The public API uses the 2018 version of the dataset.</p>"},{"location":"datasets/emod2018/#attribution","title":"Attribution","text":"<p>EMODnet Bathymetry Consortium (2018): EMODnet Digital Bathymetry (DTM).</p>"},{"location":"datasets/etopo1/","title":"ETOPO1","text":""},{"location":"datasets/etopo1/#overview","title":"Overview","text":"<p>ETOPO1 is a global elevation dataset developed by NOAA. Unlike many DEM datasets, ETOPO1 contains bathymetry (water depth). There are two variants of the dataset, which vary in how elevation is calculated for the Antartic and Greenland ice sheets: an ice-surface variant, and a bedrock-level variant.</p> <p>The dataset has a 1 arc-minute resolution, which corresponds to a resolution of about 1.8km at the equator. </p> <p>ETOPO1 was made by aggregating many other datasets. The bulk of the land elevation comes from SRTM30, while most bathymetry is sourced from GEBCO. The comprising datasets were normalised to the same vertical datum (sea level) and horizontal datum (WGS84).</p>"},{"location":"datasets/etopo1/#accuracy","title":"Accuracy","text":"<p>The accuracy of ETOPO1 varies spatially depending on the underlying source data. NOAA estimates the vertical accuracy is no better than 10m.</p> <p>The quality of ETOPO1 is high: there are no missing values or holes (holes in the SRTM30 source were fixed by hand). Transitions between source datasets are smooth.</p> <p> Pseudocolour render of ETOPO1 elevation. </p>"},{"location":"datasets/etopo1/#adding-etopo1-to-open-topo-data","title":"Adding ETOPO1 to Open Topo Data","text":"<p>Download the grid-registered <code>.tif</code> file from noaa.gov to the <code>data</code> directory and unzip. </p> <pre><code>mkdir ./data/etopo1\nwget -P ./data/etopo1 https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO1/data/ice_surface/grid_registered/georeferenced_tiff/ETOPO1_Ice_g_geotiff.zip\nunzip ./data/etopo1/ETOPO1_Ice_g_geotiff.zip\nrm ./data/etopo1/ETOPO1_Ice_g_geotiff.zip\n</code></pre> <p>The provided <code>.tif</code> file doesn't include projection information, which is needed for Open Topo Data. It can be added with GDAL:</p> <pre><code>gdal_translate -a_srs EPSG:4326 ./data/etopo1/ETOPO1_Ice_g_geotiff.tif ./data/etopo1/ETOPO1.tif\nrm ./data/etopo1/ETOPO1_Ice_g_geotiff.tif\n</code></pre> <p>Create a file <code>config.yaml</code> with the following contents</p> <pre><code>datasets:\n- name: etopo1\npath: data/etopo1/\n</code></pre> <p>Rebuild to enable the new dataset at localhost:5000/v1/etopo1?locations=27.98,86.92</p> <pre><code>make build &amp;&amp; make run\n</code></pre>"},{"location":"datasets/etopo1/#public-api","title":"Public API","text":"<p>The Open Topo Data public API lets you query ETOPO1 for free:</p> <pre><code>curl https://api.opentopodata.org/v1/etopo1?locations=39.747114,-104.996334\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"elevation\": 1596.0, \"location\": {\n\"lat\": 39.747114, \"lng\": -104.996334\n},\n\"dataset\": \"etopo1\"\n}\n], \"status\": \"OK\"\n}\n</code></pre> <p>Open Topo Data hosts the ice-elevation version of the dataset (the same as seen in the image above).</p>"},{"location":"datasets/etopo1/#attribution","title":"Attribution","text":"<p>doi:10.7289/V5C8276M</p>"},{"location":"datasets/eudem/","title":"EU-DEM","text":"<p>EU-DEM is an elevation dataset covering Europe at a 25 metre resolution.</p> <p>The dataset was created by merging elevation data from the SRTM and ASTER global datasets, as well as from Soviet topo maps at high latitudes. The datum used is EVRS2000.</p>"},{"location":"datasets/eudem/#coverage","title":"Coverage","text":"<p>The dataset covers European Environment Agency member states, plus some countries to the east. Coverage extends to small parts of Northern Africa. Unlike SRTM, EU-DEM includes the Scandinavian regions north of 60\u00b0.</p> <p> Render of EU-DEM elevation. </p>"},{"location":"datasets/eudem/#accuracy","title":"Accuracy","text":"<p>The stated vertical accuracy is \u00b1 7m RMSE. Differences to SRTM and ASTER typically fall within this 7m range even with the datasets using slightly different vertical datums. Elevations for Lake Geneva, Switzerland are 370m, 374m, and 372m for SRTM, ASTER, and EU-DEM respectively.</p>"},{"location":"datasets/eudem/#coastline","title":"Coastline","text":"<p>EU-DEM uses <code>NODATA</code> values for elevations over seas and oceans, where both ASTER and SRTM assign these areas an elevation of 0m. This means that Open Topo Data isn't able to interpolate elevations for locations very close to the coast and will return a value of <code>NaN</code> in places where SRTM and ASTER might return a 0m or 1m elevation.</p> <p>The advantage of the <code>NODATA</code> oceans is that you cane use EU-DEM without clipping to a coastline shapefile.</p>"},{"location":"datasets/eudem/#adding-eu-dem-to-open-topo-data","title":"Adding EU-DEM to Open Topo Data","text":"<p>Make a new folder for the dataset:</p> <pre><code>mkdir ./data/eudem\n</code></pre> <p>Download the dataset from Copernicus. There are 27 files. Unzip them and move all the <code>.TIF</code> files into the data folder (you don't need the <code>.aux.xml</code>, <code>.ovr</code>, or <code>.TFw</code> files).</p> <p>Your data folder should now contain only 27 TIF files:</p> <pre><code>ls ./data/eudem\n\n# eu_dem_v11_E00N20.TIF\n# eu_dem_v11_E10N00.TIF\n# eu_dem_v11_E10N10.TIF\n# ...\n</code></pre> <p>If you have gdal installed, the easiest thing to do here is build a VRT - a single raster file that links to the 27 tiles and which Open Topo Data can treat as a single-file dataset.</p> <pre><code>mkdir ./data/eudem-vrt\ncd ./data/eudem-vrt\ngdalbuildvrt -tr 25 25 -tap -te 0 0 8000000 6000000 eudem.vrt ../eudem/*.TIF\ncd ../../\n</code></pre> <p>The <code>tr</code>, <code>tap</code>, and <code>te</code> options in the above command ensure that slices from the VRT will use the exact values and grid of the source rasters.</p> <p>Then create a <code>config.yaml</code> file:</p> <pre><code>datasets:\n- name: eudem25m\npath: data/eudem-vrt/\n</code></pre> <p>Finally, rebuild to enable the new dataset at localhost:5000/v1/eudem25m?locations=51.575,-3.220.</p> <pre><code>make build &amp;&amp; make run\n</code></pre> <p>Avoiding gdal</p> <p>If you don't have gdal installed, you can use the tiles directly. There are instructions for this here, but because the EU-DEM tiles don't come with an overlap you will get a <code>null</code> elevation at locations within 0.5 pixels of tile edges. </p>"},{"location":"datasets/eudem/#buffering-tiles-optional","title":"Buffering tiles (optional)","text":"<p>The tiles provided by EU-DEM don't overlap and cover slightly less than a 1000km square. This means you'll get a <code>null</code> result for coordinates along the tile edges.</p> <p>The <code>.vrt</code> approach above solves the overlap issue, but for improved performance you can leave the tiles separate and add a buffer to each one. This is the code I used on the public API to do this:</p> <pre><code>import os\nfrom glob import glob\nimport subprocess\nimport rasterio\n# Prepare paths.\ninput_pattern = 'data/eudem/*.TIF'\ninput_paths = sorted(glob(input_pattern))\nassert input_paths\nvrt_path = 'data/eudem-vrt/eudem.vrt'\noutput_dir = 'data/eudem-buffered/'\nos.makedirs(output_dir, exist_ok=True)\n# EU-DEM specific options.\ntile_size = 1_000_000\nbuffer_size = 50\nfor input_path in input_paths:\n# Get tile bounds.\nwith rasterio.open(input_path) as f:\nbottom = int(f.bounds.bottom)\nleft = int(f.bounds.left)\n# For EU-DEM only: round this partial tile down to the nearest tile_size.\nif left == 943750:\nleft = 0\n# New tile name in SRTM format.\noutput_name = 'N' + str(bottom).zfill(7) + 'E' + str(left).zfill(7) + '.TIF'\noutput_path = os.path.join(output_dir, output_name)\n# New bounds.\nxmin = left - buffer_size\nxmax = left + tile_size + buffer_size\nymin = bottom - buffer_size\nymax = bottom + tile_size + buffer_size\n# EU-DEM tiles don't cover negative locations.\nxmin = max(0, xmin)\nymin = max(0, ymin)\n# Do the transformation.\ncmd = [\n'gdal_translate',\n'-a_srs', 'EPSG:3035',  # EU-DEM crs.\n'-co', 'NUM_THREADS=ALL_CPUS',\n'-co', 'COMPRESS=DEFLATE',\n'-co', 'BIGTIFF=YES',\n'--config', 'GDAL_CACHEMAX','512',\n'-projwin', str(xmin), str(ymax), str(xmax), str(ymin),\nvrt_path, output_path,\n]\nr = subprocess.run(cmd)\nr.check_returncode()\n</code></pre> <p>These new files can be used in Open Topo Data with the following <code>config.yaml</code> file</p> <pre><code>datasets:\n- name: eudem25m\npath: data/eudem-buffered/\nfilename_epsg: 3035\nfilename_tile_size: 1000000\n</code></pre> <p>and rebuilding:</p> <pre><code>make build &amp;&amp; make run\n</code></pre>"},{"location":"datasets/eudem/#public-api","title":"Public API","text":"<p>The Open Topo Data public API lets you query EU-DEM for free:</p> <pre><code>curl https://api.opentopodata.org/v1/eudem25m?locations=57.688709,11.976404\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"elevation\": 54.576168060302734,\n\"location\": {\n\"lat\": 57.688709,\n\"lng\": 11.976404\n},\n\"dataset\": \"eudem25m\"\n}\n],\n\"status\": \"OK\"\n}\n</code></pre> <p>Open Topo Data hosts version 1.1 of the dataset.</p>"},{"location":"datasets/eudem/#attribution","title":"Attribution","text":"<p>Released by Copernicus under the following terms:</p> <p>Access to data is based on a principle of full, open and free access as established by the Copernicus data and information policy Regulation (EU) No 1159/2013 of 12 July 2013. This regulation establishes registration and licensing conditions for GMES/Copernicus users. Free, full and open access to this data set is made on the conditions that:</p> <ol> <li> <p>When distributing or communicating Copernicus dedicated data and Copernicus service information to the public, users shall inform the public of the source of that data and information.</p> </li> <li> <p>Users shall make sure not to convey the impression to the public that the user's activities are officially endorsed by the Union.</p> </li> <li> <p>Where that data or information has been adapted or modified, the user shall clearly state this.</p> </li> <li> <p>The data remain the sole property of the European Union. Any information and data produced in the framework of the action shall be the sole property of the European Union. Any communication and publication by the beneficiary shall acknowledge that the data were produced \"with funding by the European Union\".</p> </li> </ol>"},{"location":"datasets/gebco2020/","title":"GEBCO 2020 Bathymetry","text":"<p>GEBCO maintains a high-quality, global bathymetry (sea floor depth) dataset.</p> <p>GEBCO releases a new dataset most years, the 2020 dataset (released in May 2020) covers the entire globe at a 15 arc-second resolution, corresponding to 450m resolution at the equator.</p>"},{"location":"datasets/gebco2020/#coverage","title":"Coverage","text":"<p>Elevation is given for land areas, largely using a 15-degree version of SRTM.</p> <p>Seafloor data comes from a variety of bathymetric sources, see GEBCO for more details.</p> <p> Render of GEBCO 2020 elevation. </p>"},{"location":"datasets/gebco2020/#adding-gebco-2020-to-open-topo-data","title":"Adding GEBCO 2020 to Open Topo Data","text":"<p>Instructions are given for the 2020 version of the dataset: future versions might work a bit differently.</p> <p>Make a new folder for the dataset:</p> <pre><code>mkdir ./data/gebco2020\n</code></pre> <p>Download the dataset from GEBCO. You'll want the <code>GEBCO_2020 Grid</code> version, in <code>Data GeoTiff</code> format. Extract raster tiles from the archive and delete everything else so there are just 8 <code>.tif</code> files in the <code>./data/gebco2020</code> folder.</p> <p>The files are given as 90 degree tiles, we need to rename them to SRTM's <code>NxxSxx</code> format to work with Open Topo Data: <pre><code>mv gebco_2020_n0.0_s-90.0_w0.0_e90.0.tif     S90E000.tif\nmv gebco_2020_n0.0_s-90.0_w-180.0_e-90.0.tif S90W180.tif\nmv gebco_2020_n0.0_s-90.0_w-90.0_e0.0.tif    S90W090.tif\nmv gebco_2020_n0.0_s-90.0_w90.0_e180.0.tif   S90E090.tif\nmv gebco_2020_n90.0_s0.0_w0.0_e90.0.tif      N00E000.tif\nmv gebco_2020_n90.0_s0.0_w-180.0_e-90.0.tif  N00W180.tif\nmv gebco_2020_n90.0_s0.0_w-90.0_e0.0.tif     N00W090.tif\nmv gebco_2020_n90.0_s0.0_w90.0_e180.0.tif    N00E090.tif\n</code></pre></p> <p>Create a <code>config.yaml</code> file:</p> <pre><code>datasets:\n- name: gebco2020\npath: data/gebco2020/\nfilename_tile_size: 90\n</code></pre> <p>Rebuild to enable the new dataset at localhost:5000/v1/gebco2020.</p> <pre><code>make build &amp;&amp; make run\n</code></pre>"},{"location":"datasets/gebco2020/#buffering-tiles","title":"Buffering tiles","text":"<p>The tiles provided by GEBCO don't overlap and cover slightly less than a 90\u00b0 x 90\u00b0 square. This means you'll get a <code>null</code> result for coordinates along the tile edges (like <code>0,0</code>). </p> <p>For the public API I used the following code to add a 5px buffer to each tile.</p> <pre><code>from glob import glob\nimport os\nimport rasterio\nold_folder = 'gebco_2020_geotiff'\nnew_folder = 'gebco_2020_buffer'\nbuffer_ = 5\nold_pattern = os.path.join(old_folder, '*.tif')\nold_paths = list(glob(old_pattern))\ncmd = 'gdalbuildvrt {}/all.vrt'.format(old_folder) + ' '.join(old_paths)\nos.system(cmd)\nfor path in old_paths:\nnew_path = path.replace(old_folder, new_folder)\nwith rasterio.open(path) as f:\nnew_bounds = (\nf.bounds.left - buffer_ * f.res[0],\nf.bounds.bottom - buffer_ * f.res[1],\nf.bounds.right + buffer_ * f.res[0],\nf.bounds.top + buffer_ * f.res[1],\n)\nnew_shape = (\nf.shape[0] + buffer_ * 2,\nf.shape[1] + buffer_ * 2,\n)\nte = ' '.join(str(x) for x in new_bounds)\nts = ' '.join(str(x) for x in new_shape)\ncmd = f'gdalwarp -te {te} -ts {ts} -r near -co NUM_THREADS=ALL_CPUS -co COMPRESS=DEFLATE  -co PREDICTOR=2 -co BIGTIFF=yes {old_folder}/all.vrt {new_path}'\nos.system(cmd)\n</code></pre>"},{"location":"datasets/gebco2020/#public-api","title":"Public API","text":"<p>The Open Topo Data public API lets you query GEBCO 2020 for free:</p> <pre><code>curl https://api.opentopodata.org/v1/gebco2020?locations=37.6535,-119.4105\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"elevation\": 3405.0, \"location\": {\n\"lat\": 37.6535, \"lng\": -119.4105\n},\n\"dataset\": \"gebco2020\"\n}\n], \"status\": \"OK\"\n}\n</code></pre> <p>The public API uses the 2020 version of the dataset.</p>"},{"location":"datasets/gebco2020/#attribution","title":"Attribution","text":"<p>GEBCO released the dataset into the public domain under the following terms:</p> <ul> <li>Acknowledge the source of The GEBCO Grid. A suitable form of attribution is given in the documentation that accompanies The GEBCO Grid.</li> <li>Not use The GEBCO Grid in a way that suggests any official status or that GEBCO, or the IHO or IOC, endorses any particular application of The GEBCO Grid.</li> <li>Not mislead others or misrepresent The GEBCO Grid or its source.</li> </ul>"},{"location":"datasets/mapzen/","title":"Mapzen","text":"<p>Mapzen's terrain tiles are a global DEM dataset, including bathymetry. The dataset is an assimilation of multiple open datasets. </p>"},{"location":"datasets/mapzen/#coverage-and-resolution","title":"Coverage and resolution","text":"<p>Data is provided at a 1 arc-second resolution, corresponding to a resolution of about 30m at the equator. However, parts of the dataset are interpolated from lower-resolution datasets. The resolution of the source datasets is shown below:</p> <p> Resolution of Mapzen source datasets. Source: github.com/tilezen/joerd. </p>"},{"location":"datasets/mapzen/#adding-mapzen-to-open-topo-data","title":"Adding Mapzen to Open Topo Data","text":"<p>Make a new folder for the dataset:</p> <pre><code>mkdir ./data/mapzen\n</code></pre> <p>Download the tiles from AWS. I found it easiest to use the aws cli:</p> <pre><code>aws s3 cp --no-sign-request --recursive s3://elevation-tiles-prod/skadi ./data/mapzen\n</code></pre> <p>Extract all the <code>.hgt</code> files. Create a <code>config.yaml</code> file:</p> <pre><code>datasets:\n- name: mapzen\npath: data/mapzen/\n</code></pre> <p>Rebuild to enable the new dataset at localhost:5000/v1/mapzen.</p> <pre><code>make build &amp;&amp; make run\n</code></pre> <p>Extra performance</p> <p><code>.hgt</code> files are extremely large. You'll get a large space reduction with little read penalty by converting to a compressed geotiff:</p> <pre><code>gdal_translate -co COMPRESS=DEFLATE -co PREDICTOR=2 {hgt_filename} {tif_filename}\n</code></pre>"},{"location":"datasets/mapzen/#public-api","title":"Public API","text":"<p>The Open Topo Data public API lets you query the Mapzen dataset for free:</p> <pre><code>curl https://api.opentopodata.org/v1/mapzen?locations=57.688709,11.976404\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"elevation\": 55.0, \"location\": {\n\"lat\": 57.688709, \"lng\": 11.976404\n},\n\"dataset\": \"mapzen\"\n}\n], \"status\": \"OK\"\n}\n</code></pre> <p>The public API uses Version 1.1 of Mapzen, downloaded from AWS on May 2020</p>"},{"location":"datasets/ned/","title":"NED","text":"<p>The National Elevation Dataset (NED) is a collection of DEMs covering the USA at different resolutions. </p>"},{"location":"datasets/ned/#resolution-and-coverage","title":"Resolution and Coverage","text":"<p>NED comes in several different resolutions, each with a different coverage area.</p> <p>The most commonly used resolutions are 1 arcsecond (covering North America and Mexico) and 1/3 arcsecond (covering CONUS, HI, PR, and parts of AK). The 1/3 arcsecond dataset is used in the Open Topo Data public API.</p> 1 arcsecond (30m). 1/3 arcsecond (10m). <p>Two higher resolutions have partial coverage focused on more urbanised areas.</p> 1/9 arcsecond (3m). 1m. <p>And there are separate datasets with full coverage of Alaska at 2 arseconds (60m) and 5m.</p> 2 arcsecond (60m). 5m. <p>Coverage screenshots are from The National Map.</p>"},{"location":"datasets/ned/#adding-ned-10m-to-open-topo-data","title":"Adding NED 10m to Open Topo Data","text":"<p>Make a new folder for the dataset:</p> <pre><code>mkdir ./data/ned10m\n</code></pre> <p>Download the files from USGS into <code>./data/ned10m</code>. You want the <code>USGS_13_xxxxxxx.tif</code> files.</p> <p>Next, Open Topo Data needs the filenames to match the SRTM format: the filename should be the coordinates of the lower-left corner. Here's the Python code I used to do the conversion.</p> <pre><code>from glob import glob\nimport os\nimport re\nold_pattern = './data/ned10m/USGS_13_*.tif'\nold_paths = list(glob(old_pattern))\nprint('Found {} files'.format(len(old_paths)))\nfor old_path in old_paths:\nfolder = os.path.dirname(old_path)\nold_filename = os.path.basename(old_path)\n# Extract northing.\nres = re.search(r'([ns]\\d\\d)', old_filename)\nold_northing = res.groups()[0]\n# Fix the NS \nn_or_s = old_northing[0]\nns_value = int(old_northing[1:3])\nif old_northing[:3] == 'n00':\nnew_northing = 's01' + old_northing[3:]\nelif n_or_s == 'n':\nnew_northing = 'n' + str(ns_value - 1).zfill(2) + old_northing[3:]\nelif n_or_s == 's':\nnew_northing = 's' + str(ns_value + 1).zfill(2) + old_northing[3:]\nnew_filename = old_filename.replace(old_northing, new_northing)\nassert new_northing in new_filename\n# Prevent new filename from overwriting old tiles.\nparts = new_filename.split('.')\nparts[0] = parts[0] + '_renamed'\nnew_filename = '.'.join(parts)\n# Rename in place.\nnew_path = os.path.join(folder, new_filename)\nos.rename(old_path, new_path)\n</code></pre> <p>Create a <code>config.yaml</code> file:</p> <pre><code>datasets:\n- name: ned10m\npath: data/ned10m/\nfilename_epsg: 4269\n</code></pre> <p>Rebuild to enable the new dataset at localhost:5000/v1/ned10m.</p> <pre><code>make build &amp;&amp; make run\n</code></pre>"},{"location":"datasets/ned/#public-api","title":"Public API","text":"<p>The Open Topo Data public API lets you query NED 10m for free:</p> <pre><code>curl https://api.opentopodata.org/v1/ned10m?locations=37.6535,-119.4105\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"elevation\": 3498.298583984375, \"location\": {\n\"lat\": 37.6535, \"lng\": -119.4105\n},\n\"dataset\": \"ned10m\"\n}\n], \"status\": \"OK\"\n}\n</code></pre> <p>NED is still being updated by USGS. The dataset used by the public API was last updated 2020-04-23.</p>"},{"location":"datasets/nzdem/","title":"NZ DEM","text":"<p>The 8m NZ DEM is an interpolation of the 20m contours on the 1:50,000 scale LINZ topo maps.</p>"},{"location":"datasets/nzdem/#coverage","title":"Coverage","text":"<p>The dataset covers all of New Zealand except Chatham Island at an 8 metre resolution.</p> <p> NZ DEM elevation rendering. </p>"},{"location":"datasets/nzdem/#adding-nz-dem-to-open-topo-data","title":"Adding NZ DEM to Open Topo Data","text":"<pre><code>mkdir ./data/nzdem8m\n</code></pre> <p>As of May 2020, the 8m dataset could only be painstaking downloaded a single tile at a time through the LINZ web interface. If you'd rather not do this, here are all the files as  I downloaded them from LINZ on May 2020 nzdem-may-2020.zip.</p> <p>Once you've obtained the 115 files, unzip the zip archives and delete anything without a <code>.tif</code> extension. </p> <p>For Open Topo Data to understand the grid arrangement of the files, they need to be renamed to the coordinates of the lower-left corner. Here's the Python script I used, I'm also adding a buffer to help with interpolation near tile borders:</p> <pre><code>import os\nfrom glob import glob\nfolder = './data/nzdem8m'\n# Build vrt for all tifs.\npattern = os.path.join(folder, '*.tif')\ntif_paths = list(glob(pattern))\nvrt_path = os.path.join(folder, 'all.vrt')\nassert not os.system('gdalbuildvrt {} {}'.format(vrt_path, ' '.join(tif_paths)))\nbuffer_ = 5\nfor tif_path in tif_paths:\nwith rasterio.open(tif_path) as f:\nnew_bounds = (\nf.bounds.left - buffer_ * f.res[0],\nf.bounds.bottom - buffer_ * f.res[1],\nf.bounds.right + buffer_ * f.res[0],\nf.bounds.top + buffer_ * f.res[1],\n)\nnew_shape = (\nf.shape[0] + buffer_ * 2,\nf.shape[1] + buffer_ * 2,\n)\nnorthing = f.bounds.bottom\neasting = f.bounds.left\nfilename = 'N{}E{}.tif'.format(int(northing), int(easting))\nbuffer_path = os.path.join(os.path.dirname(tif_path), filename)\nte = ' '.join(str(x) for x in new_bounds)\nts = ' '.join(str(x) for x in new_shape)\ncmd = f'gdalwarp -te {te} -ts {ts} -r near -co NUM_THREADS=ALL_CPUS -co COMPRESS=DEFLATE -co PREDICTOR=3 {vrt_path} {buffer_path}'\nassert not os.system(cmd)\nassert not os.system(f'rm {vrt_path}')\n</code></pre> <p>Create a <code>config.yaml</code> file, setting the size of the tiles (65536 metres) and the projection system used (NZ tranverse mercator):</p> <pre><code>datasets:\n- name: nzdem8m\npath: data/nzdem8m/\nfilename_tile_size: 65536\nfilename_epsg: 2193\n</code></pre> <p>Rebuild to enable the new dataset at localhost:5000/v1/nzdem8m.</p> <pre><code>make build &amp;&amp; make run\n</code></pre>"},{"location":"datasets/nzdem/#public-api","title":"Public API","text":"<p>The Open Topo Data public API lets you query NZ DEM 8m for free:</p> <pre><code>curl https://api.opentopodata.org/v1/nzdem8m?locations=-37.86118,174.79974\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"elevation\": 705.4374389648438, \"location\": {\n\"lat\": -37.86118, \"lng\": 174.79974\n},\n\"dataset\": \"nzdem8m\"\n}\n], \"status\": \"OK\"\n}\n</code></pre> <p>The data files used in the public API were downloaded from LINZ May 2020.</p>"},{"location":"datasets/nzdem/#attribution","title":"Attribution","text":"<p>Released by LINZ under the (https://creativecommons.org/licenses/by/4.0/)[Creative Commons Attribution 4.0 International] licence.</p>"},{"location":"datasets/srtm/","title":"SRTM","text":""},{"location":"datasets/srtm/#overview","title":"Overview","text":"<p>SRTM is a near-global elevation dataset, with coverage from -60 to 60 degrees latitude. </p> <p>SRTM comes in multiple resolutions. The highest resolution is 1 arc-second, which corresponds to a resolution of about 30m at the equator. The 3 arc-second (90m) version is also frequently used.</p>"},{"location":"datasets/srtm/#coverage","title":"Coverage","text":"<p>SRTM has coverage from -60 to 60 degrees latitude. The dataset is released in 1 degree tiles. Ocean areas covered by a tile have an elevation of 0m. Open Topo Data will return <code>null</code> for locations not covered by a tile.</p> <p> SRTM coverage (green area). </p>"},{"location":"datasets/srtm/#downloading-30m-srtm","title":"Downloading 30m SRTM","text":"<p>Make a new folder for the dataset:</p> <pre><code>mkdir ./data/srtm30m\n</code></pre> <p>Download the files from USGS into <code>./data/srtm30m</code>. Before downloading you'll need to register an account at earthdata.nasa.gov. Using these credentials for downloading is a little tricky, but luckily Earthdata provide download scripts in multiple different languages, the Python ones worked well for me.</p> <p>You want the <code>xxxxxxx.SRTMGL1.hgt.zip</code> files. To make downloading a bit easier, here's a list of the 14,297 URLs: srtm30m_urls.txt.</p> <p>If those scripts aren't working for you, an @SamDurand (#70) had success with logging into Earthdata in your browser, then automating the browser to download the files:</p> <pre><code>import webbrowser\nimport time\nwith open(\"srtm30m_urls.txt\", \"r\") as f:\nurl_list = f.read().split(\"\\n\")\nfor i, url in enumerate(url_list):\nwebbrowser.open_new_tab(url)\nif i % 100 == 0:\ntime.sleep(5) # pause 5s every 100 it to avoid rate limiting.\n</code></pre>"},{"location":"datasets/srtm/#adding-30m-srtm-to-open-topo-data","title":"Adding 30m SRTM to Open Topo Data","text":"<p>Create a <code>config.yaml</code> file:</p> <pre><code>datasets:\n- name: srtm30m\npath: data/srtm30m/\n</code></pre> <p>Rebuild to enable the new dataset at localhost:5000/v1/srtm30m.</p> <pre><code>make build &amp;&amp; make run\n</code></pre> <p>Extra performance</p> <p><code>.hgt.zip</code> files are extremely slow for random reads. I got a 10x read speedup and a 10% size reduction from converting to a compressed geotiff:</p> <pre><code>gdal_translate -co COMPRESS=DEFLATE -co PREDICTOR=2 {hgtzip_filename} {tif_filename}\n</code></pre> <p>Unsupported file format</p> <p>If you're leaving the tiles in <code>.hgt.zip</code> format, be aware that 16 of the files are not able to be read by gdal. There are instructions for fixing those zip files.</p>"},{"location":"datasets/srtm/#adding-90m-srtm-to-open-topo-data","title":"Adding 90m SRTM to Open Topo Data","text":"<p>The process is the same as for 30m. The dataset is hosted on USGS here, and a list of the tile urls is here: srtm90m_urls.txt.</p>"},{"location":"datasets/srtm/#public-api","title":"Public API","text":"<p>The Open Topo Data public API lets you query SRTM 30m for free:</p> <pre><code>curl https://api.opentopodata.org/v1/srtm30m?locations=57.688709,11.976404\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"elevation\": 55.0, \"location\": {\n\"lat\": 57.688709, \"lng\": 11.976404\n},\n\"dataset\": \"srtm30m\"\n}\n], \"status\": \"OK\"\n}\n</code></pre> <p>as well as SRTM 90m:</p> <pre><code>curl https://api.opentopodata.org/v1/srtm90m?locations=57.688709,11.976404\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"elevation\": 55.0, \"location\": {\n\"lat\": 57.688709, \"lng\": 11.976404\n},\n\"dataset\": \"srtm90m\"\n}\n], \"status\": \"OK\"\n}\n</code></pre> <p>The public API uses Version 3 of SRTM for both resolutions.</p>"},{"location":"datasets/swisstopo/","title":"Swisstopo","text":"<p>The Swisstopo Federal Office of Topography publishes a number of datasets, including very high quality elevation data for the whole of Switzerland at 0.5m and 2m resolutions under the name swissALTI. The data is regularly updated.</p>"},{"location":"datasets/swisstopo/#adding-2m-swiss-dem-to-open-topo-data","title":"Adding 2m Swiss DEM to Open Topo Data","text":"<p>(The procedure is the same for the 0.5m product as the tiles are the same 1km extent).</p> <p>Make a new folder for the dataset:</p> <pre><code>mkdir ./data/swisstopo-2m\n</code></pre> <p>Download the urls of each tile from swisstopo. When I did this in 2021 there were 43,579 tiles.</p> <p>Download each tile. <code>wget</code> can do this. Consider doing the downloads sequentially with a rate-limit to avoid overloading their servers.</p> <pre><code>wget --input-file ch.swisstopo.swissalti3d-xxxxx.csv --no-clobber --limit-rate 1M\n</code></pre> <p>Open Topo Data needs files to be named with the lower-left corner coordinates. The files from swisstopo are named like this, but in km instead of metres. I use the following Python script to take a file named like</p> <pre><code>swissalti3d_2019_2622-1264_2_2056_5728.tif\n</code></pre> <p>and rename it to </p> <pre><code>swissalti3d_2019_2622-1264_2_2056_5728.N1264000E2622000.tif\n</code></pre> <pre><code>from glob import glob\nimport os\ndataset_folder = './data/swisstopo-2m/'\npattern = os.path.join(dataset_folder, '**', '*.tif')\npaths = sorted(glob(pattern, recursive=True))\nfor old_path in paths:\n# Extract lower-left corder from filename.\nold_filename = os.path.basename(old_path)\nextent = old_filename.split('_')[2]\nnorthing = extent.split('-')[1]\neasting = extent.split('-')[0]\n# Convert from km to m.\nnorthing = int(northing) * 1000\neasting = int(easting) * 1000\n# Build new filename.\nnew_filename = old_filename.rsplit('.', 1)[0]\nnew_filename = new_filename + f'.N{northing}E{easting}.tif'\nnew_path = os.path.join(os.path.dirname(old_path), new_filename)\n# Rename the file.\nos.rename(old_path, new_path)\n</code></pre> <p>The final step is adding the to <code>config.yaml</code>:</p> <pre><code>- name: swisstopo-2m\npath: data/swisstopo-2m/\nfilename_epsg: 2056\nfilename_tile_size: 1000\n</code></pre>"},{"location":"datasets/swisstopo/#public-api","title":"Public API","text":"<p>The Open Topo Data public server is full to the brim with elevation data at the moment! Swisstopo is first on the waiting list to be added once I upgrade the server. </p> <p>2m Swiss elevation data is included in GPXZ</p>"},{"location":"datasets/swisstopo/#attribution","title":"Attribution","text":"<p>\u00a9swisstopo </p>"},{"location":"notes/buffering-tiles/","title":"Adding a Buffer to Tiles","text":"<p>Most tiled elevation datasets have an overlap: a 1 pixel overlap lets you interpolate locations between pixels at the edge, and some datasets (like those produced by USGS) have larger datasets that give more flexibility with interpolation and resampling.</p> <p>However not all datasets come with an overlap, which makes it impossible to interpolate locations within 1 pixel of the edge of a tile without opening multiple files. Because Open Topo Data only opens one file for each location, it may return <code>null</code> for these border locations in non-overlapping datasets.</p> <p>This isn't a problem for everyone: if your tiles are large enough, only a very small fraction of pixels lie on the edges so you may never notice issues.</p> <p>But to fix this problem you can add a buffer to the tile files. I'll use EU-DEM as an example, as the tiles by default have no overlap.</p> <p>Start by downloading the 27 <code>.TIF</code> files into <code>./data/eudem</code>. You'll also need to install the gdal commandline tools.</p> <p>Next we'll make a VRT file for the rasters</p> <pre><code>mkdir ./data/eudem-vrt\ncd ./data/eudem-vrt\ngdalbuildvrt -tr 25 25 -tap -te 0 0 8000000 6000000 -co VRT_SHARED_SOURCE=0 eudem.vrt ../eudem/*.TIF\ncd ../../\n</code></pre> <p>The <code>tr</code>, <code>tap</code>, and <code>te</code> options in the above command ensure that slices from the VRT will use the exact values and grid of the source rasters.</p> <p>Now we could actually stop here: if we add the <code>eudem-vrt</code> as a single-file dataset, Open Topo Data will handle the boundaries just fine. But for a slight performance improvement we can slice the VRT back into it's original tiles with an overlap. The following code will make put buffered tiles into <code>data/eudem-buffered</code>:</p> <pre><code>import os\nfrom glob import glob\nimport subprocess\nimport rasterio\n# Prepare paths.\ninput_pattern = 'data/eudem/*.TIF'\ninput_paths = sorted(glob(input_pattern))\nassert input_paths\nvrt_path = 'data/eudem-vrt/eudem.vrt'\noutput_dir = 'data/eudem-buffered/'\nos.makedirs(output_dir, exist_ok=True)\n# EU-DEM specific options.\ntile_size = 1_000_000\nbuffer_size = 25\nfor input_path in input_paths:\n# Get tile bounds.\nwith rasterio.open(input_path) as f:\nbottom = int(f.bounds.bottom)\nleft = int(f.bounds.left)\n# For EU-DEM only: round this partial tile down to the nearest tile_size.\nif left == 943750:\nleft = 0\n# New tile name in SRTM format.\noutput_name = 'N' + str(bottom).zfill(7) + 'E' + str(left).zfill(7) + '.TIF'\noutput_path = os.path.join(output_dir, output_name)\n# New bounds.\nxmin = left - buffer_size\nxmax = left + tile_size + buffer_size\nymin = bottom - buffer_size\nymax = bottom + tile_size + buffer_size\n# EU-DEM tiles don't cover negative locations.\nxmin = max(0, xmin)\nymin = max(0, ymin)\n# Do the transformation.\ncmd = [\n'gdal_translate',\n'-a_srs', 'EPSG:3035',  # EU-DEM crs.\n'-co', 'NUM_THREADS=ALL_CPUS',\n'-co', 'COMPRESS=DEFLATE',\n'-co', 'BIGTIFF=YES',\n'--config', 'GDAL_CACHEMAX','512',\n'-projwin', str(xmin), str(ymax), str(xmax), str(ymin),\nvrt_path, output_path,\n]\nr = subprocess.run(cmd)\nr.check_returncode()\n</code></pre> <p>These new files can be used in Open Topo Data with the following <code>config.yaml</code> file</p> <pre><code>datasets:\n- name: eudem25m\npath: data/eudem-buffered/\nfilename_epsg: 3035\nfilename_tile_size: 1000000\n</code></pre> <p>after rebuilding:</p> <pre><code>make build &amp;&amp; make run\n</code></pre>"},{"location":"notes/cloud-storage/","title":"Storing Datasets in the Cloud","text":"<p>You may want to store your elevation datasets on a cloud storage provider (like AWS S3, Google Cloud Storage, or Azure Storage) instead of on a persistent local disk. This configuration can be cheaper than provisioning a server with a lot of disk space, and plays nicely with containerised workflows where services are expected to spin up quickly on standard machine types.</p> <p>Accessing datasets over http will add latency compared reading from a local disk. </p> <p>Open Topo Data doesn't currently have specific support for cloud storage, but there are a few different ways to set this up.</p> <p>Regardless of the approach you take, you probably want to convert your dataset to cloud optimised geotiffs for best performance.</p>"},{"location":"notes/cloud-storage/#mounting-on-the-host","title":"Mounting on the host","text":"<p>Tools like gcsfuse, s3fuse, and rclone let you mount cloud storage buckets, folders, and files in your local filesystem. You can then point Open Topo Data at the mounted path.</p> <p>For example, you could mount a GCS bucket like </p> <pre><code>mkdir /mnt/otd-cloud-datasets/\ngcsfuse www-opentopodata-org-public /mnt/otd-cloud-datasets/\n</code></pre> <p>set up the dataset in <code>config.yaml</code>:</p> <pre><code>datasets:\n- name: srtm-gcloud-subset\npath: data/test-srtm90m-subset/\n</code></pre> <p>then when running the docker image, include the mounted bucket as a volume:</p> <pre><code>docker run --rm -it --volume /mnt/otd-cloud-datasets/:/app/data:ro -p 5000:5000 opentopodata\n</code></pre> <p>This is the simplest set and forget way to point Open Topo Data at a dataset living in the cloud. But it requires a long-running host, and access to the host.</p>"},{"location":"notes/cloud-storage/#mounting-inside-docker","title":"Mounting inside docker","text":"<p>You could do the mounting above inside the docker container, for example by building on the Open Topo Data docker image to add e.g. rclone as a dependency and do the actual mounting.</p> <p>This lets you mount your cloud dataset without modifying the host. However, getting fuse to work inside a docker container can be tricky, and you may not have permissions to do this on some platforms (though it seems possible on GCE).</p>"},{"location":"notes/cloud-storage/#building-a-vrt","title":"Building a VRT","text":"<p>VRT files are a container format for geospatial rasters, and they support cloud storage through special file paths: for example the path <code>/vsigs/www-opentopodata-org-public/test-srtm90m-subset/N00E010.hgt</code> references the file  <code>/test-srtm90m-subset/N00E010.hgt</code> in the <code>www-opentopodata-org-public</code> bucket on Google Cloud Storage. There's a complete list of the special paths in the GDAL docs.</p> <p>Because Open Topo Data understands VRT files, we can build a VRT file wrapping all of the cloud files:</p> <pre>\ngdalbuildvrt data/gcloud/dataset.vrt /vsigs/www-opentopodata-org-public/test-srtm90m-subset/N00E010.hgt /vsigs/www-opentopodata-org-public/test-srtm90m-subset/N00E011.hgt.zip\n</pre> <p>and load this in Open Topo Data as a single file dataset:</p> <pre><code>datasets:\n- name: srtm-gcloud-subset\npath: data/gcloud/\n</code></pre> <p>finally, you'll need to pass credentials to the docker container:</p> <pre>\ndocker run -it -v /home/XXX/opentopodata/data:/app/data:ro -p 5000:5000 -e GS_SECRET_ACCESS_KEY=XXX -e GS_ACCESS_KEY_ID=XXX opentopodata\n</pre> <p>Again the GDAL docs have the format for the credential environment variables.</p>"},{"location":"notes/dataset-sizes/","title":"Dataset sizes","text":"<p>The table below lists the file sizes of the datasets on the public APIs server.</p> <p>In most cases I converted the source files to compressed geotiffs, and the sizes given are after that conversion. A freshly downloaded dataset could be much larger depending on the format, but compressing with gdal should result in a size similar to what I ended up with.</p> API id Dataset Compressed File Size Notes bkg200 BKG 200m, Germany 32\u00a0MB etopo1 ETOPO 1 arcminute land and bathymetry 1\u00a0GB gebco2020 GEBCO global bathymetry (2020) 3\u00a0GB emod2018 EMOD Europe bathymetry (2018) 3\u00a0GB After conversion from <code>.asc</code> to compressed <code>.geotiff</code>. nzdem8m New Zealand 8m DEM 9\u00a0GB srtm90m SRTM ~90m 12\u00a0GB After conversion from <code>.hgt</code> to compressed <code>.geotiff</code>. eudem25m Europe 25m DEM 22\u00a0GB srtm30m SRTM ~30m 73\u00a0GB mapzen Mapzen ~30m 142\u00a0GB After conversion from <code>.hgt</code> format to compressed <code>.geotiff</code>. The source files are 100s of GB larger. aster30m ASTER ~30m 151\u00a0GB ned10m US National Elevation Dataset (1/3 arcsecond) 260\u00a0GB"},{"location":"notes/invalid-srtm-zips/","title":"Invalid SRTM zips over the Caspian sea","text":"<p>If you're working with SRTM tiles in <code>.hgt.zip</code> format you might have found some files that can't be processed by gdal (and therefore can't be processed by Open Topo Data):</p> <pre><code>gdalinfo N44E049.SRTMGL1.hgt.zip\n</code></pre> <pre><code>ERROR 4: `N44E049.SRTMGL1.hgt.zip' not recognized as a supported file format.\ngdalinfo failed - unable to open 'N44E049.SRTMGL1.hgt.zip'.\n</code></pre> <p>There are 16 tiles with this issue</p> <pre><code>N37E051\nN37E052\nN38E050\nN38E051\nN38E052\nN39E050\nN39E051\nN40E051\nN41E050\nN41E051\nN42E049\nN42E050\nN43E048\nN43E049\nN44E048\nN44E049\n</code></pre> <p>all of which cover no-land areas of the Caspian Sea</p> <p> Invalid tiles shown in pink. Basemap is Bing aerial imagery. </p> <p>and have a constant elevation of -29m</p> <pre><code>gdalinfo -mm N44E049.hgt | grep Min\n</code></pre> <pre><code>Computed Min/Max=-29.000,-29.000\n</code></pre>"},{"location":"notes/invalid-srtm-zips/#whats-the-issue","title":"What's the issue","text":"<p>Normal <code>.hgt.zip</code> files are a zip archive containing a file named like <code>NxxEyyy.hgt</code></p> <pre><code>unzip -l N37E011.SRTMGL1.hgt.zip\n</code></pre> <pre><code>Archive:  N37E011.SRTMGL1.hgt.zip\n  Length      Date    Time    Name\n---------  ---------- -----   ----\n 25934402  2012-10-08 15:44   N37E011.hgt\n---------                     -------\n 25934402                     1 file\n</code></pre> <p>But the zips over the Caspian sea contain a file named like <code>NxxEyyy.SRTMGL1.hgt</code></p> <pre><code>unzip -l N44E049.SRTMGL1.hgt.zip\n</code></pre> <pre><code>Archive:  N44E049.SRTMGL1.hgt.zip\n  Length      Date    Time    Name\n---------  ---------- -----   ----\n 25934402  2015-08-10 10:24   N44E049.SRTMGL1.hgt\n---------                     -------\n 25934402                     1 file\n</code></pre> <p>and unfortunately while GDAL will read <code>NxxEyyy[.something].hgt</code> and <code>NxxEyyy[.something].hgt.zip</code> files just fine, the <code>.hgt</code> file inside the <code>.zip</code> file must be named <code>NxxEyyy.hgt</code>.</p> <p>Because these tiles all have a constant value (the elevation of the Caspian sea, which is about -29m) and weren't even included in the previous version of SRTM, I'm guessing the files went through a different pipeline to the rest of the datasets. I'm not sure if this is a bug with gdal or the dataset itself.</p>"},{"location":"notes/invalid-srtm-zips/#how-to-fix-these-files","title":"How to fix these files","text":"<p>The 16 invalid tiles can be fixed by renaming the <code>.hgt</code> file contained within:</p> <pre><code>unzip N44E049.SRTMGL1.hgt.zip\nmv N44E049.SRTMGL1.hgt N44E049.hgt\nzip N44E049.SRTMGL1.repacked.hgt.zip N44E049.hgt\n</code></pre>"},{"location":"notes/invalid-srtm-zips/#which-datasets-are-affected","title":"Which datasets are affected","text":"<p>Both the 30m and 90m resolutions of SRTM version 3 hosted on usgs.gov/MEASURES are affected.</p> <p>Version 2 of SRTM just excludes the 16 tiles over the Caspian sea: </p> <p> dds.cr.usgs.gov/srtm/version2_1/SRTM3/Eurasia/ </p> <p>The Cigar version 4.1 of SRTM has the correct elevation.</p>"},{"location":"notes/kubernetes/","title":"How to deploy on Kubernetes","text":"<p>This example shows how to deploy to Kubernetes using the base opentopodata docker image. It includes a workload (ingress.yml) configuration and a service (service.yml) configuration which is used to access the opentopodata API by routing queries to the container.</p>"},{"location":"notes/kubernetes/#prerequisites","title":"Prerequisites","text":"<p>You need access to a K8s cluster and in this example we are using the command-line-interface <code>kubectl</code> to deploy. For instructions on how to do that visit the K8s website.</p>"},{"location":"notes/kubernetes/#example","title":"Example","text":"<p>Assuming you have a domain, <code>subdomain.example.com</code> where you want to make the opentopodata available on the endpoint <code>subdomain.example.com/dem-api/</code>, the files deployment.yml, ingress.yml and service.yml shows how to set that up. It works out of the box with the base opentopodata docker image.</p> <p>Simply run these commands:</p> <pre><code>kubectl --namespace my-namespace apply -f service.yml\nkubectl --namespace my-namespace apply -f ingress.yml\nkubectl --namespace my-namespace apply -f deployment.yml\n</code></pre>"},{"location":"notes/kubernetes/#serviceyml","title":"service.yml","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: dem-api\nlabels:\nservice: dem-api\nspec:\nselector:\ndeploy: dem-api\nports:\n- port: 5000\ntargetPort: 5000\n</code></pre>"},{"location":"notes/kubernetes/#ingressyml","title":"ingress.yml","text":"<pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\nname: dem-api\nannotations:\nnginx.ingress.kubernetes.io/rewrite-target: /$1\nnginx.ingress.kubernetes.io/ssl-redirect: \"false\"\nspec:\nrules:\n- host: subdomain.example.com\nhttp:\npaths:\n- path: /dem-api/(.*)\nbackend:\nserviceName: dem-api\nservicePort: 5000\n</code></pre>"},{"location":"notes/kubernetes/#deploymentyml","title":"deployment.yml","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: dem-api\nspec:\nreplicas: 1\nselector:\nmatchLabels:\ndeploy: dem-api\ntemplate:\nmetadata:\nlabels:\ndeploy: dem-api\nspec:\ncontainers:\n- image: opentopodata\nname: dem-api\nimagePullPolicy: Always\nports:\n- containerPort: 5000\nrestartPolicy: Always\n</code></pre>"},{"location":"notes/multiple-datasets/","title":"Querying multiple datasets","text":"<p>From v1.5.0, Open Topo Data has the ability to request the \"best\" elevation from multiple datasets in a single query.</p> <p>I'm still improving the functionality, please open an issue if you find a problem.</p> <p>Warning</p> <p>While Open Topo Data lets you query multiple datasets seamlessly, it doesn't ensure that the datasets you're using share the same vertical datum or have seamless transitions.</p> <p>A great usecase for Multi Datasets would be if your usage is mostly focussed on an area with contiguous DEM coverage (like the island nation of New Zealand), but you want to technically be able to query worldwide. </p> <p>A bad usecase for Multi Datasets is layering a bunch of patchy 10cm lidar data on top of a 1.8km base DEM and using it to draw small scale elevation profiles.</p> <p>Say your <code>config.yaml</code> looks like this:</p> <pre><code># Hi-res New Zealand.\n- name: nzdem8m\npath: data/nzdem8m/\nfilename_tile_size: 65536\nfilename_epsg: 2193\n# Mapzen global.\n- name: mapzen\npath: data/mapzen/\n</code></pre> <p>You want the following:</p> <ul> <li>If you query a location in New Zealand, return the hi-res nzdem result.</li> <li>If you query a location in New Zealand and nzdem is <code>null</code> at that location, return the mapzen result for that location.</li> <li>If you query a location outside New Zealand, return the mapzen result for that location.</li> </ul> <p>There are two ways to do this.</p>"},{"location":"notes/multiple-datasets/#putting-multiple-datasets-in-the-url","title":"Putting multiple datasets in the url","text":"<pre><code>curl https://api.opentopodata.org/v1/nzdem8m,mapzen?locations=-43.801,172.968|-18.143,178.444\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"dataset\": \"nzdem8m\", \"elevation\": 1.4081547260284424, \"location\": {\n\"lat\": -43.801, \"lng\": 172.968\n}\n}, {\n\"dataset\": \"mapzen\", \"elevation\": 23.0, \"location\": {\n\"lat\": -18.143, \"lng\": 178.444\n}\n}\n], \"status\": \"OK\"\n}\n</code></pre> <p>For each location, the first non-null dataset elevation is returned.</p>"},{"location":"notes/multiple-datasets/#defining-multi-dataset-in-configyaml","title":"Defining multi dataset in <code>config.yaml</code>","text":"<p>If you have a lot of datasets you want to merge, it's a pain to put them all in the url. Instead you can define a MultiDataset in <code>config.yaml</code>:</p> <pre><code># Hi-res New Zealand.\n- name: nzdem8m\npath: data/nzdem8m/\nfilename_tile_size: 65536\nfilename_epsg: 2193\n# Mapzen global.\n- name: mapzen\npath: data/mapzen/\n# NZ with mapzen fallback.\n- name: nz-global\nchild_datasets:\n- nzdem8m\n- mapzen\n</code></pre> <p>Now you can query nz-global for the same result.</p> <pre><code>curl https://api.opentopodata.org/v1/nz-global?locations=-43.801,172.968|-18.143,178.444\n</code></pre> <pre><code>{\n\"results\": [\n{\n\"dataset\": \"nzdem8m\", \"elevation\": 1.4081547260284424, \"location\": {\n\"lat\": -43.801, \"lng\": 172.968\n}\n}, {\n\"dataset\": \"mapzen\", \"elevation\": 23.0, \"location\": {\n\"lat\": -18.143, \"lng\": 178.444\n}\n}\n], \"status\": \"OK\"\n}\n</code></pre>"},{"location":"notes/multiple-datasets/#performance-optimisation","title":"Performance optimisation","text":"<p>Querying multiple datasets is more performance-intensive than querying a single one. For a Multi Dataset with 10 child datasets where the first 9 are <code>null</code> at the queried location, Open Topo Data will have to cover the lat/lon coordinates into the CRS of all 10 datasets to try to find a tile. Worst case, if those 10 tiles exist, Open Topo Data will have to open and read from all 10 files sequentially.</p> <p>To reduce the number of checks Open Topo Data has to do, you can manually specify the bounds of each dataset, in WGS84 format. If a location is outside those bounds, Open Topo Data doesn't need to check that dataset.</p> <p>If your <code>config.yaml</code> file looks like</p> <pre><code># Hi-res New Zealand.\n- name: nzdem8m\npath: data/nzdem8m/\nfilename_tile_size: 65536\nfilename_epsg: 2193\nwgs84_bounds:\nleft: 165\nright: 180\nbottom: -48\ntop: -33\n# Mapzen global.\n- name: mapzen\npath: data/mapzen/\n</code></pre> <p>then querying <code>nzdem8m,mapzen</code> with a location in Europe will go straight to mapzen without needing to convert the location to <code>epsg:2193</code> and check for a tile.</p>"},{"location":"notes/performance-optimisation/","title":"Performance optimisation","text":"<p>If you're self-hosting Open Topo Data and want better throughput or to run on a cheaper instance there's a few things you can do.</p>"},{"location":"notes/performance-optimisation/#server-hardware","title":"Server hardware","text":"<p>Open Topo Data is mostly CPU bound. It benefits from fast CPUs and high virtual core count.</p> <p>A spinning hard drive is enough to saturate the CPU so an SSD is not needed. Using a network filestore or keeping tiles in cloud storage will probably introduce enough latency to become the bottleneck.</p> <p>It uses very little memory.</p>"},{"location":"notes/performance-optimisation/#queries","title":"Queries","text":"<p>Batch request are faster (per point queried) than single-point requests, and large batches are faster than small ones. Increase <code>max_locations_per_request</code> to as much as you can fit in a a URL.</p> <p>Batch queries are fastest if the points are located next to each other. Sorting the locations you are querying before batching will improve performance. Ideally sort by some block-level attribute like postal code or state/county/region, or by something like  <code>round(lat, 1), round(lon, 1)</code> depending on your tile size.</p>"},{"location":"notes/performance-optimisation/#dataset-format","title":"Dataset format","text":"<p>A request spends 90% of its time reading the dataset, so the format your raster tiles are in can greatly impact performance. </p> <p>The most optimal way to store rasters depends heavily on the dataset, but here's some rules of thumb:</p> <ul> <li>Tile size Files around 2,000 to 20,000 are pixels square are good. Too small and you end up opening lots of files for batch requests; too large and read time is slower.</li> <li>Format Actually this one is clear: use GeoTIFFs. I haven't found anything faster (supported by GDAL) for any of the datasets in the public API.</li> <li>Compression GDAL options<ul> <li><code>-co PREDICTOR=1</code> is often fastest to read, though can make files larger.</li> <li><code>-co COMPRESS=DEFLATE</code> is often fastest to read, though can be larger than <code>lzw</code> and <code>zstd</code> especially for floating point data.</li> <li><code>-co ZLEVEL=1</code> gives a small read performance boost, makes writing noticeably faster, while barely increasing size. </li> <li>All of the above are minor differences compared to using uncompressed GeoTIFFs or other formats, don't stress it.</li> </ul> </li> </ul> <p>Open Topo Data doesn't support zstd (as it's not supported yet by rasterio and compiling GDAL from source greatly increases build times) but there's an old branch <code>zstd</code> that has support</p>"},{"location":"notes/performance-optimisation/#multiple-datasets","title":"Multiple datasets","text":"<p>Add tight <code>wgs84_bounds</code> for multiple datasets. If your dataset isn't a filled rectangle (e.g., you have one dataset covering CONUS, AK, and HI but not Canada) you might want to split it into multiple datasets with tight bounds.</p>"},{"location":"notes/performance-optimisation/#version","title":"Version","text":"<p>Open Topo Data gets faster each release, either though performance improvements or from updated dependencies.</p> <p>Use the latest version.</p> <p>This is especially true if you're using a version older than 1.5.0, as this release gives a 2x+ speedup.</p>"},{"location":"notes/performance-optimisation/#testing-performance","title":"Testing performance","text":"<p>You can easily test throughput using ab:</p> <pre><code>ab -n 500 -c 8 http://localhost:5000/v1/test-dataset?locations=56,123\n</code></pre> <p>You should test on your particular dataset and batch size. It doesn't seem to matter much if you use a fixed url or build a list with different urls for each request: there's no response caching (though your OS may cache files and GDAL may cache raster blocks.)</p>"},{"location":"notes/performance-optimisation/#benchmark-results","title":"Benchmark results","text":"<p>Here are some plots I made benchmarking version 1.5.0 with 8m NZ DEM. The specific results probably won't apply to your dataset, and I included the general takeaways above.  </p> <p> Response time grows sublinearly with batch size. Querying locations that lie on the same tile is 2x faster than locations over multiple tiles. </p> <p> Read time and file size for different GeoTIFF compression methods. </p>"},{"location":"notes/running-without-docker/","title":"Running without Docker","text":"<p>Open Topo Data uses docker to manage dependencies, builds, and processes. Containerisation is especially helpful in the geospatial domain, where compatibility between system libraries, compiled packages, and python scripts is flakey. </p> <p>So I highly recommend running Open Topo Data with docker, it saves me a bunch of headaches. But if you've read this far you already know that \ud83d\udc09.</p>"},{"location":"notes/running-without-docker/#running-open-topo-data-150-on-debian-10","title":"Running Open Topo Data 1.5.0 on Debian 10","text":"<p>A user (thanks Luca!) was able to get Open Topo Data running on Debian 10 without docker and was kind enough to share their instructions.</p>"},{"location":"notes/running-without-docker/#minimal-install","title":"Minimal install","text":"<p>Download Open Topo Data.</p> <pre><code>git clone https://github.com/ajnisbet/opentopodata.git\ncd opentopodata\n</code></pre> <p>Install system dependencies</p> <pre><code>apt install gcc python3.7-dev python3-pip\n</code></pre> <p>Debian 10 comes with an old version of pip, it needs to be updated:</p> <pre><code>pip3 install --upgrade pip\n</code></pre> <p>For some reason <code>pyproj</code> needs to be installed on its own, otherwise it will use the outdated system PROJ library instead of the packaged wheel version. Find the version of <code>pyproj</code> required</p> <pre><code>cat requirements.txt | grep pyproj\n</code></pre> <p>and install that pinned version</p> <pre><code>pip3 install pyproj==3.0.0.post1\n</code></pre> <p>then the remaining python packages can be installed:</p> <pre><code>pip3 install -r requirements.txt\n</code></pre> <p>This should give a minimal install of Open Topo Data that can be started with </p> <pre><code>FLASK_APP=opentopodata/api.py DISABLE_MEMCACHE=1 flask run --port 5000\n</code></pre>"},{"location":"notes/running-without-docker/#full-install","title":"Full install","text":"<p>The minimal instructions above install Open Topo Data without memcache or a web server. This is fine if you have a small dataset, few requests per second, and don't expose the insecure flask server to the internet. </p> <p>For a faster and more secure server, you can install memcache and uwsgi, and run the service with systemd.</p> <p>Install some more dependencies:</p> <pre><code>apt install memcached\npip3 install regex uwsgi\n</code></pre> <p>Set up memcached. On Debian 10, memcached comes with \"PrivateTemp\" enabled, which prevents saving the socket where Open Topo Data expects it:</p> <pre><code>usermod -g www-data memcache\nmkdir -p /etc/systemd/system/memcached.service.d/\necho -e \"[Service]\\n\\nPrivateTmp=false\" &gt; /etc/systemd/system/memcached.service.d/override.conf\nsystemctl daemon-reload\necho -e \"-s /tmp/memcached.sock\\n-a 0775\\n-c 1024\\n-I 8m\" &gt;&gt; /etc/memcached.conf\nservice memcached restart\n</code></pre> <p>Create a file <code>uwsgi.ini</code> somewhere, say <code>/home/opentopodata/uwsgi.ini</code>, that points to the repo you downloaded:</p> <pre><code>[uwsgi]\nstrict = true\nneed-app = true\nhttp-socket = :9090\nvacuum = true\nuid = www-data\ngid = www-data\nmaster = true\nchdir = /home/opentopodata\npythonpath = /home/opentopodata\nwsgi-file = /home/opentopodata/opentopodata/api.py\ncallable = app\nmanage-script-name = true\ndie-on-term = true\nbuffer-size = 65535\n</code></pre> <p>If uwsgi works with  <pre><code>/usr/local/bin/uwsgi --ini /home/opentopodata/uwsgi.ini --processes 10s\n</code></pre></p> <p>Then you can create a systemd script in <code>/etc/systemd/system/opentopodata.service</code>:</p> <pre><code>[Unit]\nDescription=OpenTopoData web application\nAfter=network.target\n[Service]\nUser=www-data\nWorkingDirectory=/home/opentopodata\nExecStart=/usr/local/bin/uwsgi /home/opentopodata/uwsgi.ini --processes 10s\nRestart=always\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Then manage Open Topo Data with</p> <pre><code>systemctl daemon-reload \nsystemctl enable opentopodata.service \nsystemctl start opentopodata.service \n</code></pre>"},{"location":"notes/windows-support/","title":"Running Open Topo Data on Windows","text":"<p>A few users have mentioned that the getting started instructions don't work on Windows. This is difficult for me to debug without access Windows machine, but I believe the instructions below will work.</p>"},{"location":"notes/windows-support/#getting-started-on-windows","title":"Getting started on Windows","text":"<p>Install docker and git.</p> <p>Make sure docker is running. The command <code>docker ps</code> should print a (possibly empty) table of containers, rather than an error message.</p> <p>Clone the repository with</p> <pre><code>git clone https://github.com/ajnisbet/opentopodata.git\n</code></pre> <p>and change into the repo folder</p> <pre><code>cd opentopodata\n</code></pre> <p>To build the docker image, instead of using of <code>make build</code>, build with </p> <pre><code>docker build --tag opentopodata --file docker/Dockerfile .\n</code></pre> <p>To run the server, instead of using <code>make run</code>, run with</p> <pre><code>docker run --rm -it --volume \"C:/path/to/opentopodata/data:/app/data:ro\" -p 5000:5000 -e N_UWSGI_THREADS=8 opentopodata sh -c \"/usr/bin/supervisord -c /app/docker/supervisord.conf\"\n</code></pre> <p>Modify <code>-e N_UWSGI_THREADS=8</code> in the <code>docker run</code> command above with the number of logical CPU cores on your system. Open Topo Data is CPU bound for most compressed datasets.</p> <p>Also modify <code>C:/path/to/opentopodata/data</code> in that command with the path to the folder containing your datasets.</p>"},{"location":"notes/windows-support/#troubleshooting","title":"Troubleshooting","text":""},{"location":"notes/windows-support/#error-during-connect","title":"Error during connect","text":"<p>An error like </p> <pre>\nerror during connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.40/containers/json: open //./pipe/docker_engine: The system cannot find the file specified. In the default daemon configuration on Windows, the docker client must be run elevated to connect. This error may also indicate that the docker daemon is not running.\n</pre> <p>means docker isn't running.</p>"},{"location":"notes/windows-support/#the-input-device-is-not-a-tty","title":"The input device is not a TTY","text":"<p>Trying to run the <code>docker run</code> command in Git CMD gives me this error:</p> <pre>\nthe input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'\n</pre> <p>Run the <code>docker run</code> command in Command Prompt instead.</p>"},{"location":"notes/windows-support/#could-not-find-config-file","title":"Could not find config file","text":"<p>Errors complaining about not finding <code>/app/docker/supervisord.conf</code> like</p> <pre>\nprocess_begin: CreateProcess(NULL, pwd, ...) failed.\nMakefile:8: pipe: No such file or directory\nError: could not find config file /app/docker/supervisord.conf\nFor help, use /usr/bin/supervisord -h\nmake: *** [Makefile:8: run] Error 2\n</pre> <p>or </p> <pre>\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused \"exec: \\\"exec /usr/bin/supervisord -c /app/docker/supervisord.conf\\\": stat exec /usr/bin/supervisord -c /app/docker/supervisord.conf: no such file or directory\": unknown.\n</pre> <p>should be fixed in v1.3.1 and greater, or might mean you're using <code>make</code> instead of the commands above.</p>"}]}